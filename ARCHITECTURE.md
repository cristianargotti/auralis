# AURALIS â€” AI Architecture & Development Guide
# This document is designed for AI agents to understand and continue development.

## Identity
- **Name:** AURALIS
- **Tagline:** "Hear deeper. Create beyond."
- **Type:** AI Music Production Engine
- **Language:** Python 3.12 + TypeScript (Next.js 15)
- **Deployment:** 100% cloud â€” AWS EC2 `g5.xlarge` (NVIDIA A10G GPU)
- **Access:** Web browser at `http://<ec2-ip>:3000` (UI) + `:8000` (API)

## Mission
AURALIS deconstructs any professional track into its atoms, understands every
element with AI, and reconstructs it from scratch. It also creates brand new
tracks from natural language descriptions via LLM orchestration.

## Architecture â€” 7 Layers

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  WEB UI                      â”‚
â”‚  Next.js 15 Â· shadcn/ui Â· wavesurfer.js     â”‚
â”‚  Pages: Dashboard, Deconstructor, Creator,  â”‚
â”‚         Studio, Mixer, Master Suite, QC, AI â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                 API LAYER                    â”‚
â”‚  FastAPI Â· WebSocket Â· Pydantic models      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                             â”‚
â”‚  EAR          HANDS        CONSOLE          â”‚
â”‚  Analysis     Synthesis    Mix & Master     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”‚
â”‚  Â· Demucs     Â· DawDreamer Â· Pedalboard     â”‚
â”‚  Â· RoFormer   Â· TorchFX    Â· matchering     â”‚
â”‚  Â· basic-pitchÂ· DDSP       Â· convergence    â”‚
â”‚  Â· librosa    Â· Stable     Â· NVIDIA FX      â”‚
â”‚  Â· essentia     Audio                       â”‚
â”‚                                             â”‚
â”‚  GRID          BRAIN        QC              â”‚
â”‚  Composition   AI Engine    Quality         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€      â”‚
â”‚  Â· mido        Â· OpenAI     Â· fingerprint   â”‚
â”‚  Â· musicpy       GPT        Â· comparator    â”‚
â”‚  Â· Magenta     Â· Decision   Â· convergence   â”‚
â”‚  Â· YuE           Engine     Â· 12-dim score  â”‚
â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Directory Structure

```
~/code/auralis/
â”œâ”€â”€ auralis/                    # Python package
â”‚   â”œâ”€â”€ __init__.py             # Version: 0.1.0
â”‚   â”œâ”€â”€ config.py               # Pydantic settings (env vars)
â”‚   â”œâ”€â”€ ear/                    # ğŸ‘‚ Analysis & Deconstruction
â”‚   â”‚   â”œâ”€â”€ separator.py        # Demucs source separation (GPU)
â”‚   â”‚   â”œâ”€â”€ midi_extractor.py   # basic-pitch MIDI transcription
â”‚   â”‚   â”œâ”€â”€ spectral.py         # librosa deep analysis (10 bands, MFCC, key)
â”‚   â”‚   â””â”€â”€ profiler.py         # Track DNA map (loudness, sections, dynamics)
â”‚   â”œâ”€â”€ hands/                  # ğŸ¹ Synthesis & Sound Design
â”‚   â”‚   â”œâ”€â”€ vst_host.py         # DawDreamer VST2/VST3 host
â”‚   â”‚   â”œâ”€â”€ torchfx_dsp.py      # GPU-accelerated DSP (DAFx25)
â”‚   â”‚   â”œâ”€â”€ ddsp_synth.py       # Google DDSP differentiable synth
â”‚   â”‚   â”œâ”€â”€ stable_audio.py     # Stability AI text-to-audio
â”‚   â”‚   â””â”€â”€ faust_dsp.py        # Custom Faust DSP modules
â”‚   â”œâ”€â”€ console/                # ğŸšï¸ Mixing & Mastering
â”‚   â”‚   â”œâ”€â”€ fx.py               # Pedalboard FX chains + VST hosting
â”‚   â”‚   â”œâ”€â”€ mixer.py            # Buses, sends, EQ, compression
â”‚   â”‚   â”œâ”€â”€ mastering.py        # Reference-matched convergence loop
â”‚   â”‚   â””â”€â”€ dsp/                # Custom: Moog filter, bitcrush, sidechain
â”‚   â”œâ”€â”€ grid/                   # ğŸ“ Composition & Arrangement
â”‚   â”‚   â”œâ”€â”€ midi.py             # MIDI read/write/generate
â”‚   â”‚   â”œâ”€â”€ theory.py           # Music theory (musicpy)
â”‚   â”‚   â”œâ”€â”€ arrangement.py      # Section-based track building
â”‚   â”‚   â””â”€â”€ yue_gen.py          # YuE full-song generation
â”‚   â”œâ”€â”€ brain/                  # ğŸ§  AI Intelligence
â”‚   â”‚   â”œâ”€â”€ agent.py            # OpenAI GPT orchestrator
â”‚   â”‚   â””â”€â”€ production_ai.py    # Decision engine for production
â”‚   â”œâ”€â”€ qc/                     # ğŸ” Quality Assurance
â”‚   â”‚   â”œâ”€â”€ fingerprint.py      # Per-band spectral fingerprint
â”‚   â”‚   â”œâ”€â”€ comparator.py       # A/B track comparison
â”‚   â”‚   â”œâ”€â”€ convergence.py      # Mastering convergence loop
â”‚   â”‚   â””â”€â”€ musical_review.py   # 12-dimension scoring
â”‚   â””â”€â”€ api/                    # âš¡ FastAPI Backend
â”‚       â”œâ”€â”€ server.py           # Main app + route registration
â”‚       â”œâ”€â”€ websocket.py        # Real-time progress (ConnectionManager)
â”‚       â””â”€â”€ routes/
â”‚           â””â”€â”€ ear.py          # POST /upload, /analyze, GET /status
â”œâ”€â”€ web/                        # ğŸŒ Next.js 15 Frontend (TODO)
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_api.py             # Health check + config tests
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ quality-check.sh        # SEC + LINT + FORMAT + TYPES + TESTS
â”œâ”€â”€ .github/workflows/ci.yml    # GitHub Actions CI pipeline
â”œâ”€â”€ pyproject.toml              # UV project config
â”œâ”€â”€ .env.example                # Environment variable template
â”œâ”€â”€ .gitleaks.toml              # Secret scanning config
â””â”€â”€ README.md
```

## API Reference

### REST Endpoints
| Method | Path | Description |
|--------|------|-------------|
| GET | `/health` | Health check â†’ `{"status": "ok"}` |
| GET | `/api/info` | System capabilities and layer descriptions |
| POST | `/api/ear/upload` | Upload audio file (WAV/MP3/FLAC/AIFF) |
| POST | `/api/ear/analyze/{project_id}` | Start analysis pipeline |
| GET | `/api/ear/status/{job_id}` | Poll job progress |
| GET | `/api/ear/models` | List separation models |

### WebSocket
| Path | Description |
|------|-------------|
| `WS /ws/{project_id}` | Real-time progress: `{type, step, total, percentage, message}` |

## Data Models

### SpectralProfile (ear/spectral.py)
10-band frequency analysis, MFCC, chroma, key/scale estimation, tempo,
beat tracking, harmonic ratio, RMS energy. Used for A/B comparison.

### TrackDNA (ear/profiler.py)
Complete track identity: key, scale, tempo, EBU R128 loudness (LUFS),
true peak, loudness range, crest factor, dynamic range, sections with
energy levels, and full spectral profile. Serializable to JSON.

### SeparationResult (ear/separator.py)
Paths to separated stems (vocals, drums, bass, other), model metadata,
sample rate, duration. Supports Demucs HTDemucs, HTDemucs-FT, MDX Extra.

### MIDIExtractionResult (ear/midi_extractor.py)
MIDI file path, note count, pitch range, duration, confidence score.
Batch processing of all tonal stems.

## Technology Stack

### Currently Installed (core)
- `librosa` â€” Audio analysis, spectral features, beat tracking
- `pyloudnorm` â€” EBU R128 loudness measurement
- `soundfile` â€” Audio I/O (WAV, FLAC, OGG)
- `numpy`, `scipy` â€” Numeric computing
- `mido` â€” MIDI read/write
- `openai` â€” LLM integration
- `fastapi` + `uvicorn` â€” HTTP/WebSocket API
- `pydantic` + `pydantic-settings` â€” Data validation + env config
- `structlog` â€” Structured logging
- `ruff` â€” Linting + formatting
- `mypy` â€” Strict type checking
- `pytest` â€” Testing + coverage

### Optional (installed on EC2 with GPU)
- `[ml]`: `torch`, `demucs` â€” Source separation, ML inference
- `[audio]`: `pedalboard`, `matchering` â€” FX processing, mastering

### Planned (not yet installed)
- `essentia` â€” 600+ audio analysis algorithms
- `dawdreamer` â€” VST2/VST3 host in Python
- `basic-pitch` â€” Audio-to-MIDI transcription
- `musicpy` â€” Music theory engine
- `magenta` â€” AI music generation
- `TorchFX` â€” GPU-accelerated DSP
- `DDSP` â€” Differentiable synthesis
- `Stable Audio Open` â€” Text-to-audio
- `YuE` â€” Full-song generation

## Quality Standards (MeetMind-proven)

| Gate | Tool | Rule |
|------|------|------|
| SEC-001 | `gitleaks` | 0 secrets in code |
| LINT | `ruff check` | 0 errors |
| FORMAT | `ruff format` | 100% formatted |
| TYPES | `mypy --strict` | 0 type errors |
| TESTS | `pytest --cov` | â‰¥80% coverage |
| CODE-001 | line count | â‰¤500 soft / â‰¤800 hard per file |

### Commands
```bash
# Run all quality gates
bash scripts/quality-check.sh

# Individual checks
uv run ruff check auralis/ tests/
uv run ruff format --check auralis/ tests/
uv run mypy auralis/
uv run pytest --cov=auralis --tb=short -q

# Start API server
uv run uvicorn auralis.api.server:app --reload --host 0.0.0.0 --port 8000
```

## Environment Variables

```env
AURALIS_OPENAI_API_KEY=sk-...   # OpenAI API key
AURALIS_HOST=0.0.0.0            # Server bind host
AURALIS_PORT=8000               # Server bind port
AURALIS_ENV=development         # Environment
AURALIS_PROJECTS_DIR=./projects # Where projects are stored
AURALIS_SAMPLES_DIR=./samples   # Sample library
AWS_PROFILE=mibaggy-co          # AWS profile for EC2
AWS_REGION=us-east-1            # AWS region
```

## Mastering Convergence Loop (100% Match)

```
Our Mix â†’ matchering (reference EQ + RMS + width)
       â†’ Render Master
       â†’ Spectral Fingerprint vs Original
       â†’ â‰¤1% deviation per band? â†’ NO â†’ Corrective EQ â†’ Re-render
                                 â†’ YES â†’ Phase â‰¥0.95? â†’ NO â†’ Phase correction
                                                       â†’ YES â†’ LUFS Â±0.1? â†’ APPROVED
```

### Validation Thresholds
| Dimension | Metric | Target |
|-----------|--------|--------|
| Spectral | Per-band energy (10 bands) | â‰¤1% deviation |
| Dynamic | LUFS, crest factor, peak | Â±0.1 LUFS |
| Stereo | Width, correlation | Â±0.02 |
| Temporal | Beat alignment | â‰¤5ms |
| Perceptual | MFCC cosine distance | â‰¤0.05 |

## Development Workflow

### Adding a New Module
1. Create file in the appropriate layer directory
2. Add docstring explaining purpose
3. Add type hints (mypy --strict must pass)
4. Write tests in `tests/`
5. Run `bash scripts/quality-check.sh`
6. File must be â‰¤500 lines (800 hard limit)

### Adding a New API Route
1. Create route file in `auralis/api/routes/`
2. Use FastAPI `APIRouter` with prefix and tags
3. Register in `auralis/api/server.py` with `app.include_router()`
4. Add Pydantic models for request/response
5. Use `asyncio.create_task()` for long-running operations
6. Send progress via `WebSocket ConnectionManager`

### Adding a New UI Page
1. Create page in `web/app/<page-name>/page.tsx`
2. Use shadcn/ui components
3. Connect to API endpoints via fetch/axios
4. Connect to WebSocket for real-time updates
5. Follow dark theme design system

## Project Roadmap

### Phase 1 âœ… â€” Scaffolding + EAR [CURRENT]
- Project structure, quality gates, CI pipeline
- EAR layer: spectral analysis, profiling, separation, MIDI extraction
- API server with routes and WebSocket
- Deploy to EC2

### Phase 2 â€” HANDS + CONSOLE
- Synthesis engine (DawDreamer + Surge XT + TorchFX + DDSP)
- FX engine (Pedalboard + custom DSP)
- Mixing engine (buses, sends, EQ, compression)
- Mastering convergence loop

### Phase 3 â€” GRID + BRAIN + Creator
- MIDI/composition engine
- LLM orchestrator (OpenAI GPT)
- Creator page: describe track â†’ LLM produces everything

### Phase 4 â€” Million Pieces Reconstruction
- Full deconstruction + recreation

### Phase 5 â€” Mono Aullador Reconstruction
- Rebuild production pipeline inside AURALIS

## Key Design Decisions
1. **Graceful degradation**: Heavy ML deps are optional â€” core runs without GPU
2. **Async everything**: FastAPI + asyncio for non-blocking audio processing
3. **WebSocket progress**: Real-time updates for long-running operations
4. **JSON metadata**: Every operation saves metadata for reproducibility
5. **10-band spectral comparison**: Foundation for convergence mastering
6. **EBU R128 loudness**: Industry-standard loudness measurement
7. **Section detection**: Automatic arrangement mapping for reconstruction
