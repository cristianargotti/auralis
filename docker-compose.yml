services:
  api:
    build:
      context: .
      dockerfile: Dockerfile.api
    env_file: .env
    environment:
      - AURALIS_JWT_SECRET=auralis-persistent-secret-2026-fixed-v1
      # Auto-scaling: Let PyTorch/Uvicorn detect available cores
    restart: unless-stopped
    volumes:
      - ml_models:/root/.cache    # Persist HTDemucs model weights (~320MB)
      - projects_data:/app/projects  # Persist separated stems & audio
      - samples_data:/data/samples   # Organic sample packs (bind mount)
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/health')"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  web:
    build:
      context: .
      dockerfile: Dockerfile.web
    environment:
      - NEXT_PUBLIC_API_URL=
    restart: unless-stopped
    depends_on:
      api:
        condition: service_healthy

  caddy:
    image: caddy:2-alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile
      - caddy_data:/data
      - caddy_config:/config
    restart: unless-stopped
    depends_on:
      - api
      - web

volumes:
  caddy_data:
  caddy_config:
  ml_models:      # HTDemucs + torch hub weights
  projects_data:  # Stems, renders, mixes
  samples_data:   # Organic sample packs
